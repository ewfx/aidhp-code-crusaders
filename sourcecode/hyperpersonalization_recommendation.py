# -*- coding: utf-8 -*-
"""HyperPersonalization_Recommendation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/174sEHzaSetaemtchkkA_gR5jim5uWl-5
"""

from pydantic import BaseModel
import torch

# This part of code will skip all the un-necessary warnings which can occur during the execution of this project.
import warnings
warnings.filterwarnings("ignore", category=Warning)

# Installation for GPU llama-cpp-python==0.2.69
!CMAKE_ARGS="-DLLAMA_CUDA=on" pip install llama-cpp-python==0.2.69
# For downloading the models from HF Hub
!pip install huggingface_hub

# !pip uninstall llama-cpp-python
# CMAKE_ARGS="-DLLAMA_CUBLAS=on"
# !pip install --force-reinstall llama-cpp-python --no-cache-dir

import json

import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split

from huggingface_hub import hf_hub_download
from llama_cpp import Llama

from tqdm import tqdm
from collections import Counter

# Load Mistral 7B model (Placeholder for actual model loading)
def load_model():
    model_name_or_path = "TheBloke/Mistral-7B-Instruct-v0.2-GGUF"
    model_basename = "mistral-7b-instruct-v0.2.Q5_K_M.gguf"
    model_path = hf_hub_download(
              repo_id=model_name_or_path,
              filename=model_basename
          )
    lcpp_llm = Llama(

            model_path=model_path,
            n_threads=2, # CPU cores
            n_batch=512, # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.
            n_gpu_layers=43, # Change this value based on your model and your GPU VRAM pool.
            n_ctx=4096 # Context window
        )
    return lcpp_llm

model = load_model()

!pip install flask flask-ngrok
!pip install pyngrok

from pydantic import BaseModel
from flask import Flask, jsonify, request
from flask_ngrok import run_with_ngrok  # Ensure you have flask-ngrok installed
from pyngrok import ngrok

app = Flask(__name__)
run_with_ngrok(app)  # Enables ngrok

!ngrok authtoken 2flPCqXtK0mJarxJ8qKyWAdyP8P_5jAr786C4PjUnW9QWZ2kZ

class PredictionRequest(BaseModel):
  message: str
  max_tokens: int = 200

mistral_prompt_template = """<s>[INST]{prompt}[/INST]"""

answers_template = """
Context:
{context}
===
Using the context above generate {num_answers} distinct answers to the following question:
Question:
{question}.

Arrange your answers in numbered bullet points.
Present only the answers in bullet points.
"""

def generateModelResponse(user_query, user_query_context):
  answers_prompt = mistral_prompt_template.format(
      prompt=answers_template.format(
          context=user_query_context,
          question=user_query,
          num_answers=3
      )
  )

  response = model(
    prompt=answers_prompt,
    max_tokens=1024,
    temperature=0,
    top_p=0.95,
    repeat_penalty=1.2,
    echo=False # do not return the prompt
  )
  factual_answers = response["choices"][0]["text"].strip()
  return factual_answers

{"Predictive insights": generateModelResponse("I'm very happy with the product", "You are an AI assistant that performs sentiment analysis.")}



@app.post("/api/v1/predictiveInsight")
def predictive_insights():
    input = request.get_json()
    system_message = "You are an AI assistant that generates predictive insights."
    response = generateModelResponse(input.get("message", ""), system_message)
    return {"Predictive insights": response}

@app.post("/api/v1/sentiment")
def sentiment_analysis():
    input = request.get_json()
    system_message = "You are an AI assistant that performs sentiment analysis."
    response = generateModelResponse(input.get("message", ""), system_message)
    return {"User sentiment": response}

@app.post("/api/v1/personalizeRecommendation/")
def personalized_recommendation():
    input = request.get_json()
    system_message = "You are an AI assistant that provide personalized recommendations."
    response = generateModelResponse(input.get("message", ""), system_message)
    return {"recommendation": response}

@app.post("/api/v1/assessRisk/")
def risk_assessment():
    input = request.get_json()
    system_message = "You are an AI assistant that performs risk assessments."
    response = generateModelResponse(input.get("message", ""), system_message)
    return {"Risk_assessment": response}

@app.route("/", methods=["GET"])
def root():
    return {"message": "AI-driven API for predictive insights, personalization, sentiment analysis, and risk assessment"}

# Open a tunnel on the port Flask is running on (default 5000)
public_url = ngrok.connect(5000)
print(" * Tunnel URL:", public_url)

if __name__ == "__main__":
    app.run()